{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re,os,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOVE_DIR = r'D:\\sudeep_work\\language_modelling\\glove.6B'\n",
    "TEXT_DATA_DIR = r\"D:\\sudeep_work\\language_modelling\\data\"\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name =  1.txt\n",
      "path =  D:\\sudeep_work\\language_modelling\\data\\1.txt\n",
      "a\n",
      "Found 1 texts.\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    print(\"name = \",name)\n",
    "    fpath = os.path.join(TEXT_DATA_DIR, name)\n",
    "    print(\"path = \",fpath)\n",
    "    print(\"a\")\n",
    "    label_id = len(labels_index)\n",
    "    labels_index[name] = label_id\n",
    "    if sys.version_info < (3,):\n",
    "        f = open(fpath)\n",
    "    else:\n",
    "        f = open(fpath, encoding='latin-1')\n",
    "    t = f.read()\n",
    "    i = t.find('\\n\\n')  # skip header\n",
    "    if 0 < i:\n",
    "        t = t[i:]\n",
    "    texts.append(t)\n",
    "    f.close()\n",
    "    labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nuse a cascade of many layers of nonlinear processing units for feature extraction and transformation.']"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 unique tokens.\n",
      "data =  [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  2  3  4  1  5  6  1  7  8  9 10 11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NUM_WORDS )\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(\"data = \",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "print(data.shape)\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding = 'utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1000)\n",
      "(?, 1000, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_19 (Embedding)     (None, 1000, 100)         1500      \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 31,751\n",
      "Trainable params: 30,251\n",
      "Non-trainable params: 1,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "print(sequence_input.shape)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "print(embedded_sequences.shape)\n",
    "model = LSTM(50)(embedded_sequences)\n",
    "preds = Dense(len(labels_index), activation='softmax')(model)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 0 samples, validate on 1 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ProgbarLogger' object has no attribute 'log_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-359-77a70171c0b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# happy learning!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m model.fit(x_train, y_train, validation_data=(x_val, y_val),\n\u001b[1;32m----> 3\u001b[1;33m           epochs=2, batch_size=128)\n\u001b[0m",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1231\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m                                 \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1234\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    304\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ProgbarLogger' object has no attribute 'log_values'"
     ]
    }
   ],
   "source": [
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use a cascade of many layers of nonlinear processing units for feature extraction and transformation.\n"
     ]
    }
   ],
   "source": [
    "with open('Corpus (2).txt', 'r',encoding = 'utf8') as cf:\n",
    "    corpus = []\n",
    "    for line in cf: # loops over all the lines in the corpus\n",
    "        line = line.strip() # strips off \\n \\r from the ends \n",
    "        if line: # Take only non empty lines\n",
    "            line = re.sub(r'\\([^)]*\\)', '', line) # Regular Expression to remove text in between brackets\n",
    "            line = re.sub(' +',' ', line) # Removes consecutive spaces\n",
    "            # add more pre-processing steps\n",
    "            corpus.append(line)\n",
    "print(\"\\n\".join(corpus[:5])) # Shows the first 5 lines of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Spacy\n",
    "import spacy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    corpus_tokens = []\n",
    "    sentence_lengths = []\n",
    "    for line in corpus:\n",
    "        doc = nlp(line) # Parse each line in the corpus\n",
    "        for sent in doc.sents: # Loop over all the sentences in the line\n",
    "            corpus_tokens.append('SEQUENCE_BEGIN')\n",
    "            s_len = 1\n",
    "            for tok in sent: # Loop over all the words in a sentence\n",
    "                if tok.text.strip() != '' and tok.ent_type_ != '': # If the token is a Named Entity then do not lowercase it \n",
    "                    corpus_tokens.append(tok.text)\n",
    "                else:\n",
    "                    corpus_tokens.append(tok.text.lower())\n",
    "                s_len += 1\n",
    "            corpus_tokens.append('SEQUENCE_END')\n",
    "            sentence_lengths.append(s_len+1)\n",
    "    return corpus_tokens, sentence_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQUENCE_BEGIN', 'use', 'a', 'cascade', 'of', 'many', 'layers', 'of', 'nonlinear', 'processing', 'units', 'for', 'feature', 'extraction', 'and', 'transformation', '.', 'SEQUENCE_END']\n",
      "Mean Sentence Length: 18.0\n",
      "Sentence Length Standard Deviation: 0.0\n",
      "Max Sentence Length: 18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus_tokens, sentence_lengths = preprocess_corpus(corpus)\n",
    "print(corpus_tokens) # Prints the first 30 tokens\n",
    "mean_sentence_length = np.mean(sentence_lengths)\n",
    "deviation_sentence_length = np.std(sentence_lengths)\n",
    "max_sentence_length = np.max(sentence_lengths)\n",
    "print('Mean Sentence Length: {}\\nSentence Length Standard Deviation: {}\\n'\n",
    "      'Max Sentence Length: {}'.format(mean_sentence_length, deviation_sentence_length, max_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(corpus_tokens)) # This works well for a very small corpus\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 17\n",
      "[('of', 2), ('SEQUENCE_BEGIN', 1), ('use', 1), ('a', 1), ('cascade', 1), ('many', 1), ('layers', 1), ('nonlinear', 1), ('processing', 1), ('units', 1), ('for', 1), ('feature', 1), ('extraction', 1), ('and', 1), ('transformation', 1), ('.', 1), ('SEQUENCE_END', 1)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for term in corpus_tokens:\n",
    "    word_counter.update({term: 1})\n",
    "word_counter\n",
    "Vocab = word_counter.most_common(500) \n",
    "print('Vocab Size: {}'.format(len(Vocab))) \n",
    "print(word_counter.most_common(100)) # just to show the top 100 terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Idx Size: 19\n",
      "Idx2Word Size: 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Vocab.append(('UNKNOWN', 1))\n",
    "Idx = range(1, len(Vocab)+1)\n",
    "vocab = [t[0] for t in Vocab]\n",
    "\n",
    "Word2Idx = dict(zip(vocab, Idx))\n",
    "Idx2Word = dict(zip(Idx, vocab))\n",
    "\n",
    "Word2Idx['PAD'] = 0\n",
    "Idx2Word[0] = 'PAD'\n",
    "VOCAB_SIZE = len(Word2Idx)\n",
    "print('Word2Idx Size: {}'.format(len(Word2Idx)))\n",
    "print('Idx2Word Size: {}'.format(len(Idx2Word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'PAD',\n",
       " 1: 'of',\n",
       " 2: 'SEQUENCE_BEGIN',\n",
       " 3: 'use',\n",
       " 4: 'a',\n",
       " 5: 'cascade',\n",
       " 6: 'many',\n",
       " 7: 'layers',\n",
       " 8: 'nonlinear',\n",
       " 9: 'processing',\n",
       " 10: 'units',\n",
       " 11: 'for',\n",
       " 12: 'feature',\n",
       " 13: 'extraction',\n",
       " 14: 'and',\n",
       " 15: 'transformation',\n",
       " 16: '.',\n",
       " 17: 'SEQUENCE_END',\n",
       " 18: 'UNKNOWN'}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Idx2Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = np.random.rand(len(Word2Idx), 384) # We use 300 because Spacy provides us with vectors of size 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token =  of\n",
      "token =  SEQUENCE_BEGIN\n",
      "token =  use\n",
      "token =  a\n",
      "token =  cascade\n",
      "token =  many\n",
      "token =  layers\n",
      "token =  nonlinear\n",
      "token =  processing\n",
      "token =  units\n",
      "token =  for\n",
      "token =  feature\n",
      "token =  extraction\n",
      "token =  and\n",
      "token =  transformation\n",
      "token =  .\n",
      "token =  SEQUENCE_END\n",
      "token =  UNKNOWN\n",
      "token =  PAD\n",
      "Shape of w2v: (19, 384)\n",
      "Some Vectors\n",
      "[-1.32558358 -0.42881754  3.4990747   2.36021471 -3.10067129  3.62956619\n",
      " -1.90712786 -3.13748407  0.44400662 -0.10318977] PAD\n",
      "[-1.32558358 -0.42881754  3.4990747   2.36021471 -3.10067129  3.62956619\n",
      " -1.90712786 -3.13748407  0.44400662 -0.10318977] feature\n"
     ]
    }
   ],
   "source": [
    "for w_i, key in enumerate(Word2Idx):\n",
    "    token = nlp(key)\n",
    "    print(\"token = \",token)\n",
    "    if token.has_vector:\n",
    "    #   print(token.text, Word2Idx[key])\n",
    "    #   print(\"token.vector = \",token.vector.shape)\n",
    "        w2v[Word2Idx[key]:] = token.vector\n",
    "EMBEDDING_SIZE = w2v.shape[-1]\n",
    "print('Shape of w2v: {}'.format(w2v.shape))\n",
    "print('Some Vectors')\n",
    "print(w2v[0][:10], Idx2Word[0])\n",
    "print(w2v[12][:10], Idx2Word[12])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 14\n",
      "Validation Size: 3\n"
     ]
    }
   ],
   "source": [
    "train_val_split = int(len(corpus_tokens) * 0.8) # We use 80% of the data for Training and 20% for validating\n",
    "train = corpus_tokens[:train_val_split]\n",
    "validation = corpus_tokens[train_val_split:-1]\n",
    "\n",
    "print('Train Size: {}\\nValidation Size: {}'.format(len(train), len(validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A method to convert a sequence of words into a sequence of IDs given a Word2Idx dictionary\n",
    "def word2idseq(data, word2idx):\n",
    "    id_seq = []\n",
    "    for word in data:\n",
    "        if word in word2idx:\n",
    "            id_seq.append(word2idx[word])\n",
    "        else:\n",
    "            id_seq.append(word2idx['UNKNOWN'])\n",
    "    return id_seq\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    print(\"n = \",n)\n",
    "    print(\"input_list = \",input_list)\n",
    "    return zip(*[input_list[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_id_seqs = word2idseq(train, Word2Idx)\n",
    "validation_id_seqs = word2idseq(validation, Word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Train IDs\n",
      "[2, 3, 4, 5, 1, 6, 7, 1, 8, 9, 10, 11, 12, 13]\n",
      "train_id_text =  ['SEQUENCE_BEGIN', 'use', 'a', 'cascade', 'of', 'many', 'layers', 'of', 'nonlinear', 'processing', 'units', 'for', 'feature', 'extraction']\n",
      "Sample Validation IDs\n",
      "[14, 15, 16]\n",
      "validation_id_seqs_text =  ['and', 'transformation', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Sample Train IDs')\n",
    "print(train_id_seqs)\n",
    "train_id_text = [Idx2Word[i] for i in train_id_seqs]\n",
    "print(\"train_id_text = \",train_id_text)\n",
    "print('Sample Validation IDs')\n",
    "print(validation_id_seqs)\n",
    "validation_id_seqs_text = [Idx2Word[i] for i in validation_id_seqs]\n",
    "print(\"validation_id_seqs_text = \",validation_id_seqs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, n_grams=5, batch_size=8, n_epochs=10):\n",
    "    dataset_X = []\n",
    "    dataset_Y = []\n",
    "    X, Y = [], []\n",
    "    buff_size, start, end = 8, 0, 8\n",
    "    n_buffer = 0\n",
    "    epoch = 0\n",
    "    print(\"there\")\n",
    "    while epoch <= n_epochs:\n",
    "        print(\"epoch\",epoch)\n",
    "        print(\"len(X) = \",len(X))\n",
    "        if len(X) >= batch_size:\n",
    "\n",
    "            X_batch = X[:batch_size]\n",
    "            Y_batch = Y[:batch_size]\n",
    "            X_batch = pad_sequences(X_batch, maxlen=n_grams, value=0)\n",
    "            print(\"X_batch = \",X_batch)\n",
    "            print(\"Y_batch = \",Y_batch)\n",
    "            \n",
    "            Y_batch = to_categorical(Y_batch, VOCAB_SIZE)\n",
    "#             yield (X_batch, Y_batch, epoch)\n",
    "            dataset_X.extend(list(X_batch))\n",
    "            dataset_Y.extend(list(Y_batch))\n",
    "            \n",
    "            X = X[batch_size:]\n",
    "            Y = Y[batch_size:]\n",
    "            continue\n",
    "        n = random.randrange(2, n_grams)\n",
    "        print(\"n = \",n)\n",
    "        if len(data) < n: continue\n",
    "        print(\"start,end = \",start,end)\n",
    "        if end > len(data): end = len(data)\n",
    "        grams = find_ngrams(data[start: end], n) # generates the n-grams\n",
    "        print(\"grams = \",grams)\n",
    "        splits = list(zip(*grams)) # split it\n",
    "        print(\"splits = \", splits)\n",
    "        X += list(zip(*splits[:len(splits)-1])) # from the inputs\n",
    "        print(\"X = \",X)\n",
    "        X = [list(x) for x in X] \n",
    "        print(\"X = \",X)\n",
    "        Y += splits[-1] # form the targets\n",
    "        print(\"Y = \",Y)\n",
    "        if start + buff_size > len(data):\n",
    "            print(\"start + buff_size\")\n",
    "            start = 0\n",
    "            epoch += 1\n",
    "            end = start + buff_size\n",
    "        else:\n",
    "            start = start + buff_size\n",
    "            end = end + buff_size\n",
    "    return dataset_X, dataset_Y \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters# Hyperp \n",
    "LR = 0.0001\n",
    "HIDDEN_DIMS = 256\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS=10\n",
    "N_GRAMS = 5\n",
    "N_VALIDATE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 1, 6, 7, 1, 8, 9, 10, 11, 12, 13]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_id_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "epoch 0\n",
      "len(X) =  0\n",
      "n =  3\n",
      "start,end =  0 8\n",
      "n =  3\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x000000000B1EFE48>\n",
      "splits =  [(2, 3, 4, 5, 1, 6), (3, 4, 5, 1, 6, 7), (4, 5, 1, 6, 7, 1)]\n",
      "X =  [(2, 3), (3, 4), (4, 5), (5, 1), (1, 6), (6, 7)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7]]\n",
      "Y =  [4, 5, 1, 6, 7, 1]\n",
      "epoch 0\n",
      "len(X) =  6\n",
      "n =  4\n",
      "start,end =  8 16\n",
      "n =  4\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x000000000E105DC8>\n",
      "splits =  [(8, 9, 10), (9, 10, 11), (10, 11, 12), (11, 12, 13)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], (8, 9, 10), (9, 10, 11), (10, 11, 12)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 1\n",
      "len(X) =  9\n",
      "n =  4\n",
      "start,end =  0 8\n",
      "n =  4\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x000000000F02DCC8>\n",
      "splits =  [(2, 3, 4, 5, 1), (3, 4, 5, 1, 6), (4, 5, 1, 6, 7), (5, 1, 6, 7, 1)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], (2, 3, 4), (3, 4, 5), (4, 5, 1), (5, 1, 6), (1, 6, 7)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1]\n",
      "epoch 1\n",
      "len(X) =  14\n",
      "n =  2\n",
      "start,end =  8 16\n",
      "n =  2\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x000000000F032708>\n",
      "splits =  [(8, 9, 10, 11, 12), (9, 10, 11, 12, 13)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], (8,), (9,), (10,), (11,), (12,)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 2\n",
      "len(X) =  19\n",
      "n =  2\n",
      "start,end =  0 8\n",
      "n =  2\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x000000000F037488>\n",
      "splits =  [(2, 3, 4, 5, 1, 6, 7), (3, 4, 5, 1, 6, 7, 1)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], (2,), (3,), (4,), (5,), (1,), (6,), (7,)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], [2], [3], [4], [5], [1], [6], [7]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1]\n",
      "epoch 2\n",
      "len(X) =  26\n",
      "n =  2\n",
      "start,end =  8 16\n",
      "n =  2\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x000000000E3E7208>\n",
      "splits =  [(8, 9, 10, 11, 12), (9, 10, 11, 12, 13)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], [2], [3], [4], [5], [1], [6], [7], (8,), (9,), (10,), (11,), (12,)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 3\n",
      "len(X) =  31\n",
      "n =  2\n",
      "start,end =  0 8\n",
      "n =  2\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x000000000DD34188>\n",
      "splits =  [(2, 3, 4, 5, 1, 6, 7), (3, 4, 5, 1, 6, 7, 1)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12], (2,), (3,), (4,), (5,), (1,), (6,), (7,)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12], [2], [3], [4], [5], [1], [6], [7]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1]\n",
      "epoch 3\n",
      "len(X) =  38\n",
      "X_batch =  [[ 0  0  0  2  3]\n",
      " [ 0  0  0  3  4]\n",
      " [ 0  0  0  4  5]\n",
      " [ 0  0  0  5  1]\n",
      " [ 0  0  0  1  6]\n",
      " [ 0  0  0  6  7]\n",
      " [ 0  0  8  9 10]\n",
      " [ 0  0  9 10 11]\n",
      " [ 0  0 10 11 12]\n",
      " [ 0  0  2  3  4]\n",
      " [ 0  0  3  4  5]\n",
      " [ 0  0  4  5  1]\n",
      " [ 0  0  5  1  6]\n",
      " [ 0  0  1  6  7]\n",
      " [ 0  0  0  0  8]\n",
      " [ 0  0  0  0  9]\n",
      " [ 0  0  0  0 10]\n",
      " [ 0  0  0  0 11]\n",
      " [ 0  0  0  0 12]\n",
      " [ 0  0  0  0  2]\n",
      " [ 0  0  0  0  3]\n",
      " [ 0  0  0  0  4]\n",
      " [ 0  0  0  0  5]\n",
      " [ 0  0  0  0  1]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  0  7]\n",
      " [ 0  0  0  0  8]\n",
      " [ 0  0  0  0  9]\n",
      " [ 0  0  0  0 10]\n",
      " [ 0  0  0  0 11]\n",
      " [ 0  0  0  0 12]\n",
      " [ 0  0  0  0  2]]\n",
      "Y_batch =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 3]\n",
      "epoch 3\n",
      "len(X) =  6\n",
      "n =  4\n",
      "start,end =  8 16\n",
      "n =  4\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x000000000E3E7288>\n",
      "splits =  [(8, 9, 10), (9, 10, 11), (10, 11, 12), (11, 12, 13)]\n",
      "X =  [[3], [4], [5], [1], [6], [7], (8, 9, 10), (9, 10, 11), (10, 11, 12)]\n",
      "X =  [[3], [4], [5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 4\n",
      "len(X) =  9\n",
      "n =  2\n",
      "start,end =  0 8\n",
      "n =  2\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x000000000DD38C48>\n",
      "splits =  [(2, 3, 4, 5, 1, 6, 7), (3, 4, 5, 1, 6, 7, 1)]\n",
      "X =  [[3], [4], [5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], (2,), (3,), (4,), (5,), (1,), (6,), (7,)]\n",
      "X =  [[3], [4], [5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1]\n",
      "epoch 4\n",
      "len(X) =  16\n",
      "n =  2\n",
      "start,end =  8 16\n",
      "n =  2\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x000000000E105A88>\n",
      "splits =  [(8, 9, 10, 11, 12), (9, 10, 11, 12, 13)]\n",
      "X =  [[3], [4], [5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], (8,), (9,), (10,), (11,), (12,)]\n",
      "X =  [[3], [4], [5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 5\n",
      "len(X) =  21\n",
      "n =  4\n",
      "start,end =  0 8\n",
      "n =  4\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x000000000DD34588>\n",
      "splits =  [(2, 3, 4, 5, 1), (3, 4, 5, 1, 6), (4, 5, 1, 6, 7), (5, 1, 6, 7, 1)]\n",
      "X =  [[3], [4], [5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12], (2, 3, 4), (3, 4, 5), (4, 5, 1), (5, 1, 6), (1, 6, 7)]\n",
      "X =  [[3], [4], [5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 5, 1, 6, 7, 1]\n",
      "epoch 5\n",
      "len(X) =  26\n",
      "n =  2\n",
      "start,end =  8 16\n",
      "n =  2\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x000000000F02D248>\n",
      "splits =  [(8, 9, 10, 11, 12), (9, 10, 11, 12, 13)]\n",
      "X =  [[3], [4], [5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], (8,), (9,), (10,), (11,), (12,)]\n",
      "X =  [[3], [4], [5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 6\n",
      "len(X) =  31\n",
      "n =  4\n",
      "start,end =  0 8\n",
      "n =  4\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x000000000F037308>\n",
      "splits =  [(2, 3, 4, 5, 1), (3, 4, 5, 1, 6), (4, 5, 1, 6, 7), (5, 1, 6, 7, 1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[3], [4], [5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], (2, 3, 4), (3, 4, 5), (4, 5, 1), (5, 1, 6), (1, 6, 7)]\n",
      "X =  [[3], [4], [5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 5, 1, 6, 7, 1]\n",
      "epoch 6\n",
      "len(X) =  36\n",
      "X_batch =  [[ 0  0  0  0  3]\n",
      " [ 0  0  0  0  4]\n",
      " [ 0  0  0  0  5]\n",
      " [ 0  0  0  0  1]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  0  7]\n",
      " [ 0  0  8  9 10]\n",
      " [ 0  0  9 10 11]\n",
      " [ 0  0 10 11 12]\n",
      " [ 0  0  0  0  2]\n",
      " [ 0  0  0  0  3]\n",
      " [ 0  0  0  0  4]\n",
      " [ 0  0  0  0  5]\n",
      " [ 0  0  0  0  1]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  0  7]\n",
      " [ 0  0  0  0  8]\n",
      " [ 0  0  0  0  9]\n",
      " [ 0  0  0  0 10]\n",
      " [ 0  0  0  0 11]\n",
      " [ 0  0  0  0 12]\n",
      " [ 0  0  2  3  4]\n",
      " [ 0  0  3  4  5]\n",
      " [ 0  0  4  5  1]\n",
      " [ 0  0  5  1  6]\n",
      " [ 0  0  1  6  7]\n",
      " [ 0  0  0  0  8]\n",
      " [ 0  0  0  0  9]\n",
      " [ 0  0  0  0 10]\n",
      " [ 0  0  0  0 11]\n",
      " [ 0  0  0  0 12]\n",
      " [ 0  0  2  3  4]]\n",
      "Y_batch =  [4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 5]\n",
      "epoch 6\n",
      "len(X) =  4\n",
      "n =  3\n",
      "start,end =  8 16\n",
      "n =  3\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x000000000B1FC408>\n",
      "splits =  [(8, 9, 10, 11), (9, 10, 11, 12), (10, 11, 12, 13)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], (8, 9), (9, 10), (10, 11), (11, 12)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9], [9, 10], [10, 11], [11, 12]]\n",
      "Y =  [1, 6, 7, 1, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 7\n",
      "len(X) =  8\n",
      "n =  3\n",
      "start,end =  0 8\n",
      "n =  3\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x000000000B1FD788>\n",
      "splits =  [(2, 3, 4, 5, 1, 6), (3, 4, 5, 1, 6, 7), (4, 5, 1, 6, 7, 1)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9], [9, 10], [10, 11], [11, 12], (2, 3), (3, 4), (4, 5), (5, 1), (1, 6), (6, 7)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7]]\n",
      "Y =  [1, 6, 7, 1, 10, 11, 12, 13, 4, 5, 1, 6, 7, 1]\n",
      "epoch 7\n",
      "len(X) =  14\n",
      "n =  4\n",
      "start,end =  8 16\n",
      "n =  4\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x000000000E3E71C8>\n",
      "splits =  [(8, 9, 10), (9, 10, 11), (10, 11, 12), (11, 12, 13)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], (8, 9, 10), (9, 10, 11), (10, 11, 12)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12]]\n",
      "Y =  [1, 6, 7, 1, 10, 11, 12, 13, 4, 5, 1, 6, 7, 1, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 8\n",
      "len(X) =  17\n",
      "n =  2\n",
      "start,end =  0 8\n",
      "n =  2\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x000000000F032348>\n",
      "splits =  [(2, 3, 4, 5, 1, 6, 7), (3, 4, 5, 1, 6, 7, 1)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], (2,), (3,), (4,), (5,), (1,), (6,), (7,)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7]]\n",
      "Y =  [1, 6, 7, 1, 10, 11, 12, 13, 4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1]\n",
      "epoch 8\n",
      "len(X) =  24\n",
      "n =  3\n",
      "start,end =  8 16\n",
      "n =  3\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x000000000F02D248>\n",
      "splits =  [(8, 9, 10, 11), (9, 10, 11, 12), (10, 11, 12, 13)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], (8, 9), (9, 10), (10, 11), (11, 12)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8, 9], [9, 10], [10, 11], [11, 12]]\n",
      "Y =  [1, 6, 7, 1, 10, 11, 12, 13, 4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 9\n",
      "len(X) =  28\n",
      "n =  3\n",
      "start,end =  0 8\n",
      "n =  3\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x000000000B1FD488>\n",
      "splits =  [(2, 3, 4, 5, 1, 6), (3, 4, 5, 1, 6, 7), (4, 5, 1, 6, 7, 1)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8, 9], [9, 10], [10, 11], [11, 12], (2, 3), (3, 4), (4, 5), (5, 1), (1, 6), (6, 7)]\n",
      "X =  [[3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7]]\n",
      "Y =  [1, 6, 7, 1, 10, 11, 12, 13, 4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 10, 11, 12, 13, 4, 5, 1, 6, 7, 1]\n",
      "epoch 9\n",
      "len(X) =  34\n",
      "X_batch =  [[ 0  0  3  4  5]\n",
      " [ 0  0  4  5  1]\n",
      " [ 0  0  5  1  6]\n",
      " [ 0  0  1  6  7]\n",
      " [ 0  0  0  8  9]\n",
      " [ 0  0  0  9 10]\n",
      " [ 0  0  0 10 11]\n",
      " [ 0  0  0 11 12]\n",
      " [ 0  0  0  2  3]\n",
      " [ 0  0  0  3  4]\n",
      " [ 0  0  0  4  5]\n",
      " [ 0  0  0  5  1]\n",
      " [ 0  0  0  1  6]\n",
      " [ 0  0  0  6  7]\n",
      " [ 0  0  8  9 10]\n",
      " [ 0  0  9 10 11]\n",
      " [ 0  0 10 11 12]\n",
      " [ 0  0  0  0  2]\n",
      " [ 0  0  0  0  3]\n",
      " [ 0  0  0  0  4]\n",
      " [ 0  0  0  0  5]\n",
      " [ 0  0  0  0  1]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  0  7]\n",
      " [ 0  0  0  8  9]\n",
      " [ 0  0  0  9 10]\n",
      " [ 0  0  0 10 11]\n",
      " [ 0  0  0 11 12]\n",
      " [ 0  0  0  2  3]\n",
      " [ 0  0  0  3  4]\n",
      " [ 0  0  0  4  5]\n",
      " [ 0  0  0  5  1]]\n",
      "Y_batch =  [1, 6, 7, 1, 10, 11, 12, 13, 4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 10, 11, 12, 13, 4, 5, 1, 6]\n",
      "epoch 9\n",
      "len(X) =  2\n",
      "n =  3\n",
      "start,end =  8 16\n",
      "n =  3\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x000000000F037E08>\n",
      "splits =  [(8, 9, 10, 11), (9, 10, 11, 12), (10, 11, 12, 13)]\n",
      "X =  [[1, 6], [6, 7], (8, 9), (9, 10), (10, 11), (11, 12)]\n",
      "X =  [[1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12]]\n",
      "Y =  [7, 1, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 10\n",
      "len(X) =  6\n",
      "n =  4\n",
      "start,end =  0 8\n",
      "n =  4\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x000000000DD340C8>\n",
      "splits =  [(2, 3, 4, 5, 1), (3, 4, 5, 1, 6), (4, 5, 1, 6, 7), (5, 1, 6, 7, 1)]\n",
      "X =  [[1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], (2, 3, 4), (3, 4, 5), (4, 5, 1), (5, 1, 6), (1, 6, 7)]\n",
      "X =  [[1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7]]\n",
      "Y =  [7, 1, 10, 11, 12, 13, 5, 1, 6, 7, 1]\n",
      "epoch 10\n",
      "len(X) =  11\n",
      "n =  2\n",
      "start,end =  8 16\n",
      "n =  2\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x000000000E3E7608>\n",
      "splits =  [(8, 9, 10, 11, 12), (9, 10, 11, 12, 13)]\n",
      "X =  [[1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], (8,), (9,), (10,), (11,), (12,)]\n",
      "X =  [[1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12]]\n",
      "Y =  [7, 1, 10, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13]\n",
      "start + buff_size\n"
     ]
    }
   ],
   "source": [
    "dataset_X, dataset_Y  = prepare_data(train_id_seqs, N_GRAMS, BATCH_SIZE, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = dataset_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 2, 3]),\n",
       " array([0, 0, 0, 3, 4]),\n",
       " array([0, 0, 0, 4, 5]),\n",
       " array([0, 0, 0, 5, 1]),\n",
       " array([0, 0, 0, 1, 6]),\n",
       " array([0, 0, 0, 6, 7]),\n",
       " array([ 0,  0,  8,  9, 10]),\n",
       " array([ 0,  0,  9, 10, 11]),\n",
       " array([ 0,  0, 10, 11, 12]),\n",
       " array([0, 0, 2, 3, 4]),\n",
       " array([0, 0, 3, 4, 5]),\n",
       " array([0, 0, 4, 5, 1]),\n",
       " array([0, 0, 5, 1, 6]),\n",
       " array([0, 0, 1, 6, 7]),\n",
       " array([0, 0, 0, 0, 8]),\n",
       " array([0, 0, 0, 0, 9]),\n",
       " array([ 0,  0,  0,  0, 10]),\n",
       " array([ 0,  0,  0,  0, 11]),\n",
       " array([ 0,  0,  0,  0, 12]),\n",
       " array([0, 0, 0, 0, 2]),\n",
       " array([0, 0, 0, 0, 3]),\n",
       " array([0, 0, 0, 0, 4]),\n",
       " array([0, 0, 0, 0, 5]),\n",
       " array([0, 0, 0, 0, 1]),\n",
       " array([0, 0, 0, 0, 6]),\n",
       " array([0, 0, 0, 0, 7]),\n",
       " array([0, 0, 0, 0, 8]),\n",
       " array([0, 0, 0, 0, 9]),\n",
       " array([ 0,  0,  0,  0, 10]),\n",
       " array([ 0,  0,  0,  0, 11]),\n",
       " array([ 0,  0,  0,  0, 12]),\n",
       " array([0, 0, 0, 0, 2]),\n",
       " array([0, 0, 0, 0, 3]),\n",
       " array([0, 0, 0, 0, 4]),\n",
       " array([0, 0, 0, 0, 5]),\n",
       " array([0, 0, 0, 0, 1]),\n",
       " array([0, 0, 0, 0, 6]),\n",
       " array([0, 0, 0, 0, 7]),\n",
       " array([ 0,  0,  8,  9, 10]),\n",
       " array([ 0,  0,  9, 10, 11]),\n",
       " array([ 0,  0, 10, 11, 12]),\n",
       " array([0, 0, 0, 0, 2]),\n",
       " array([0, 0, 0, 0, 3]),\n",
       " array([0, 0, 0, 0, 4]),\n",
       " array([0, 0, 0, 0, 5]),\n",
       " array([0, 0, 0, 0, 1]),\n",
       " array([0, 0, 0, 0, 6]),\n",
       " array([0, 0, 0, 0, 7]),\n",
       " array([0, 0, 0, 0, 8]),\n",
       " array([0, 0, 0, 0, 9]),\n",
       " array([ 0,  0,  0,  0, 10]),\n",
       " array([ 0,  0,  0,  0, 11]),\n",
       " array([ 0,  0,  0,  0, 12]),\n",
       " array([0, 0, 2, 3, 4]),\n",
       " array([0, 0, 3, 4, 5]),\n",
       " array([0, 0, 4, 5, 1]),\n",
       " array([0, 0, 5, 1, 6]),\n",
       " array([0, 0, 1, 6, 7]),\n",
       " array([0, 0, 0, 0, 8]),\n",
       " array([0, 0, 0, 0, 9]),\n",
       " array([ 0,  0,  0,  0, 10]),\n",
       " array([ 0,  0,  0,  0, 11]),\n",
       " array([ 0,  0,  0,  0, 12]),\n",
       " array([0, 0, 2, 3, 4]),\n",
       " array([0, 0, 3, 4, 5]),\n",
       " array([0, 0, 4, 5, 1]),\n",
       " array([0, 0, 5, 1, 6]),\n",
       " array([0, 0, 1, 6, 7]),\n",
       " array([0, 0, 0, 8, 9]),\n",
       " array([ 0,  0,  0,  9, 10]),\n",
       " array([ 0,  0,  0, 10, 11]),\n",
       " array([ 0,  0,  0, 11, 12]),\n",
       " array([0, 0, 0, 2, 3]),\n",
       " array([0, 0, 0, 3, 4]),\n",
       " array([0, 0, 0, 4, 5]),\n",
       " array([0, 0, 0, 5, 1]),\n",
       " array([0, 0, 0, 1, 6]),\n",
       " array([0, 0, 0, 6, 7]),\n",
       " array([ 0,  0,  8,  9, 10]),\n",
       " array([ 0,  0,  9, 10, 11]),\n",
       " array([ 0,  0, 10, 11, 12]),\n",
       " array([0, 0, 0, 0, 2]),\n",
       " array([0, 0, 0, 0, 3]),\n",
       " array([0, 0, 0, 0, 4]),\n",
       " array([0, 0, 0, 0, 5]),\n",
       " array([0, 0, 0, 0, 1]),\n",
       " array([0, 0, 0, 0, 6]),\n",
       " array([0, 0, 0, 0, 7]),\n",
       " array([0, 0, 0, 8, 9]),\n",
       " array([ 0,  0,  0,  9, 10]),\n",
       " array([ 0,  0,  0, 10, 11]),\n",
       " array([ 0,  0,  0, 11, 12]),\n",
       " array([0, 0, 0, 2, 3]),\n",
       " array([0, 0, 0, 3, 4]),\n",
       " array([0, 0, 0, 4, 5]),\n",
       " array([0, 0, 0, 5, 1])]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = dataset_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.])]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from numpy.random import seed\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from IPython import display\n",
    "import time\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle , gc\n",
    "from fractions import Fraction\n",
    "from collections import Counter, OrderedDict\n",
    "# from Activation import relu\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate,Reshape\n",
    "from keras.optimizers import Adagrad, Adam, RMSprop\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D,Conv1D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Bidirectional\n",
    "from keras import activations\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D,MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Input\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    " from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 384)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(Vocab),\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=N_GRAMS,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 96, 5)\n",
      "(?, 96, 5, 384)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_16: expected ndim=3, found ndim=4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-306-8a9d6cc5b3a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0membedded_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    456\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_16: expected ndim=3, found ndim=4"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 96 arrays: [array([0, 0, 0, 2, 3]), array([0, 0, 0, 3, 4]), array([0, 0, 0, 4, 5]), array([0, 0, 0, 5, 1]), array([0, 0, 0, 1, 6]), array([0, 0, 0, 6, 7]), array([ 0,  0,  8,  9, 10]), array([ 0,  0,  9, 10, 11]...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-304-54d2765b3ec0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# happy learning!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m model.fit(train_X, train_Y, validation_data=(train_X, train_Y),\n\u001b[1;32m----> 8\u001b[1;33m           epochs=2, batch_size=1)\n\u001b[0m",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1555\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1556\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1407\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1410\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1411\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     87\u001b[0m                                  \u001b[1;34m'the following list of '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                                  \u001b[1;34m' arrays: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                                  '...')\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 96 arrays: [array([0, 0, 0, 2, 3]), array([0, 0, 0, 3, 4]), array([0, 0, 0, 4, 5]), array([0, 0, 0, 5, 1]), array([0, 0, 0, 1, 6]), array([0, 0, 0, 6, 7]), array([ 0,  0,  8,  9, 10]), array([ 0,  0,  9, 10, 11]..."
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(train_X, train_Y, validation_data=(train_X, train_Y),\n",
    "          epochs=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# print(input_shape.shape)\n",
    "model.add(Embedding(len(Vocab),model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=2, batch_size=128), embeddings_initializer=embedding_matrix, input_length=5))\n",
    "\n",
    "# print(model.shape)\n",
    "model.add(LSTM(50))\n",
    "# model.add(Dense(len(Vocab), activation='softmax'))\n",
    "model.add(Dense(len(Vocab), activation='softmax'))\n",
    "# print(output.shape)\n",
    "    \n",
    "# model = Model(input_shape, output)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.32558358 -0.42881754  3.4990747   2.36021471 -3.10067129  3.62956619\n",
      " -1.90712786 -3.13748407  0.44400662 -0.10318977] a\n"
     ]
    }
   ],
   "source": [
    "print(w2v[4][:10], Idx2Word[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
