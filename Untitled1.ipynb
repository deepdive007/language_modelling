{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re,os,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = r'D:\\sudeep_work\\language_modelling\\glove.6B'\n",
    "TEXT_DATA_DIR = r\"D:\\sudeep_work\\language_modelling\\data\\1.txt\"\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use a cascade of many layers of nonlinear processing units for feature extraction and transformation.\n"
     ]
    }
   ],
   "source": [
    "with open(TEXT_DATA_DIR, 'r',encoding = 'utf8') as cf:\n",
    "    corpus = []\n",
    "    for line in cf: # loops over all the lines in the corpus\n",
    "        line = line.strip() # strips off \\n \\r from the ends \n",
    "        if line: # Take only non empty lines\n",
    "            line = re.sub(r'\\([^)]*\\)', '', line) # Regular Expression to remove text in between brackets\n",
    "            line = re.sub(' +',' ', line) # Removes consecutive spaces\n",
    "            # add more pre-processing steps\n",
    "            corpus.append(line)\n",
    "print(\"\\n\".join(corpus)) # Shows the first 5 lines of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['use a cascade of many layers of nonlinear processing units for feature extraction and transformation.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Spacy\n",
    "import spacy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    corpus_tokens = []\n",
    "    sentence_lengths = []\n",
    "    for line in corpus:\n",
    "        doc = nlp(line) # Parse each line in the corpus\n",
    "        for sent in doc.sents: # Loop over all the sentences in the line\n",
    "            corpus_tokens.append('SEQUENCE_BEGIN')\n",
    "            s_len = 1\n",
    "            for tok in sent: # Loop over all the words in a sentence\n",
    "                if tok.text.strip() != '' and tok.ent_type_ != '': # If the token is a Named Entity then do not lowercase it \n",
    "                    corpus_tokens.append(tok.text)\n",
    "                else:\n",
    "                    corpus_tokens.append(tok.text.lower())\n",
    "                s_len += 1\n",
    "            corpus_tokens.append('SEQUENCE_END')\n",
    "            sentence_lengths.append(s_len+1)\n",
    "    return corpus_tokens, sentence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEQUENCE_BEGIN', 'use', 'a', 'cascade', 'of', 'many', 'layers', 'of', 'nonlinear', 'processing', 'units', 'for', 'feature', 'extraction', 'and', 'transformation', '.', 'SEQUENCE_END']\n",
      "Mean Sentence Length: 18.0\n",
      "Sentence Length Standard Deviation: 0.0\n",
      "Max Sentence Length: 18\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens, sentence_lengths = preprocess_corpus(corpus)\n",
    "print(corpus_tokens) # Prints the first 30 tokens\n",
    "mean_sentence_length = np.mean(sentence_lengths)\n",
    "deviation_sentence_length = np.std(sentence_lengths)\n",
    "max_sentence_length = np.max(sentence_lengths)\n",
    "print('Mean Sentence Length: {}\\nSentence Length Standard Deviation: {}\\n'\n",
    "      'Max Sentence Length: {}'.format(mean_sentence_length, deviation_sentence_length, max_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "Vocab = list(set(corpus_tokens)) # This works well for a very small corpus\n",
    "print(len(Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 17\n",
      "[('of', 2), ('SEQUENCE_BEGIN', 1), ('use', 1), ('a', 1), ('cascade', 1), ('many', 1), ('layers', 1), ('nonlinear', 1), ('processing', 1), ('units', 1), ('for', 1), ('feature', 1), ('extraction', 1), ('and', 1), ('transformation', 1), ('.', 1), ('SEQUENCE_END', 1)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for term in corpus_tokens:\n",
    "    word_counter.update({term: 1})\n",
    "word_counter\n",
    "Vocab = word_counter.most_common(500) \n",
    "print('Vocab Size: {}'.format(len(Vocab))) \n",
    "print(word_counter.most_common(100)) # just to show the top 100 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab =  18\n",
      "Word2Idx Size: 19\n",
      "Idx2Word Size: 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Vocab.append(('UNKNOWN', 1))\n",
    "print(\"Vocab = \",len(Vocab))\n",
    "Idx = range(1, len(Vocab)+1)\n",
    "vocab = [t[0] for t in Vocab]\n",
    "\n",
    "Word2Idx = dict(zip(vocab, Idx))\n",
    "Idx2Word = dict(zip(Idx, vocab))\n",
    "\n",
    "Word2Idx['PAD'] = 0\n",
    "Idx2Word[0] = 'PAD'\n",
    "VOCAB_SIZE = len(Word2Idx)\n",
    "print('Word2Idx Size: {}'.format(len(Word2Idx)))\n",
    "print('Idx2Word Size: {}'.format(len(Idx2Word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'PAD',\n",
       " 1: 'of',\n",
       " 2: 'SEQUENCE_BEGIN',\n",
       " 3: 'use',\n",
       " 4: 'a',\n",
       " 5: 'cascade',\n",
       " 6: 'many',\n",
       " 7: 'layers',\n",
       " 8: 'nonlinear',\n",
       " 9: 'processing',\n",
       " 10: 'units',\n",
       " 11: 'for',\n",
       " 12: 'feature',\n",
       " 13: 'extraction',\n",
       " 14: 'and',\n",
       " 15: 'transformation',\n",
       " 16: '.',\n",
       " 17: 'SEQUENCE_END',\n",
       " 18: 'UNKNOWN'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Idx2Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(Word2Idx) , EMBEDDING_DIM))# We use 300 because Spacy provides us with vectors of size 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding = 'utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_i, key =  0 of\n",
      "w_i, key =  1 SEQUENCE_BEGIN\n",
      "w_i, key =  2 use\n",
      "w_i, key =  3 a\n",
      "w_i, key =  4 cascade\n",
      "w_i, key =  5 many\n",
      "w_i, key =  6 layers\n",
      "w_i, key =  7 nonlinear\n",
      "w_i, key =  8 processing\n",
      "w_i, key =  9 units\n",
      "w_i, key =  10 for\n",
      "w_i, key =  11 feature\n",
      "w_i, key =  12 extraction\n",
      "w_i, key =  13 and\n",
      "w_i, key =  14 transformation\n",
      "w_i, key =  15 .\n",
      "w_i, key =  16 SEQUENCE_END\n",
      "w_i, key =  17 UNKNOWN\n",
      "w_i, key =  18 PAD\n"
     ]
    }
   ],
   "source": [
    "for w_i, key in enumerate(Word2Idx):\n",
    "    print(\"w_i, key = \",w_i, key)\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[w_i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 14\n",
      "Validation Size: 3\n"
     ]
    }
   ],
   "source": [
    "train_val_split = int(len(corpus_tokens) * 0.8) # We use 80% of the data for Training and 20% for validating\n",
    "train = corpus_tokens[:train_val_split]\n",
    "validation = corpus_tokens[train_val_split:-1]\n",
    "\n",
    "print('Train Size: {}\\nValidation Size: {}'.format(len(train), len(validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A method to convert a sequence of words into a sequence of IDs given a Word2Idx dictionary\n",
    "def word2idseq(data, word2idx):\n",
    "    id_seq = []\n",
    "    for word in data:\n",
    "        if word in word2idx:\n",
    "            id_seq.append(word2idx[word])\n",
    "        else:\n",
    "            id_seq.append(word2idx['UNKNOWN'])\n",
    "    return id_seq\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    print(\"n = \",n)\n",
    "    print(\"input_list = \",input_list)\n",
    "    return zip(*[input_list[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_id_seqs = word2idseq(train, Word2Idx)\n",
    "validation_id_seqs = word2idseq(validation, Word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Train IDs\n",
      "[2, 3, 4, 5, 1, 6, 7, 1, 8, 9, 10, 11, 12, 13]\n",
      "train_id_text =  ['SEQUENCE_BEGIN', 'use', 'a', 'cascade', 'of', 'many', 'layers', 'of', 'nonlinear', 'processing', 'units', 'for', 'feature', 'extraction']\n",
      "Sample Validation IDs\n",
      "[14, 15, 16]\n",
      "validation_id_seqs_text =  ['and', 'transformation', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Sample Train IDs')\n",
    "print(train_id_seqs)\n",
    "train_id_text = [Idx2Word[i] for i in train_id_seqs]\n",
    "print(\"train_id_text = \",train_id_text)\n",
    "print('Sample Validation IDs')\n",
    "print(validation_id_seqs)\n",
    "validation_id_seqs_text = [Idx2Word[i] for i in validation_id_seqs]\n",
    "print(\"validation_id_seqs_text = \",validation_id_seqs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, n_grams=5, batch_size=8, n_epochs=10):\n",
    "    dataset_X = []\n",
    "    dataset_Y = []\n",
    "    X, Y = [], []\n",
    "    buff_size, start, end = 8, 0, 8\n",
    "    n_buffer = 0\n",
    "    epoch = 0\n",
    "    print(\"there\")\n",
    "    while epoch <= n_epochs:\n",
    "        print(\"epoch\",epoch)\n",
    "        print(\"len(X) = \",len(X))\n",
    "        if len(X) >= batch_size:\n",
    "\n",
    "            X_batch = X[:batch_size]\n",
    "            Y_batch = Y[:batch_size]\n",
    "            X_batch = pad_sequences(X_batch, maxlen=n_grams, value=0)\n",
    "            print(\"X_batch = \",X_batch)\n",
    "            print(\"Y_batch = \",Y_batch)\n",
    "            \n",
    "            Y_batch = to_categorical(Y_batch, VOCAB_SIZE)\n",
    "#             yield (X_batch, Y_batch, epoch)\n",
    "            dataset_X.extend(list(X_batch))\n",
    "            dataset_Y.extend(list(Y_batch))\n",
    "            \n",
    "            X = X[batch_size:]\n",
    "            Y = Y[batch_size:]\n",
    "            continue\n",
    "        n = random.randrange(2, n_grams)\n",
    "        print(\"n = \",n)\n",
    "        if len(data) < n: continue\n",
    "        print(\"start,end = \",start,end)\n",
    "        if end > len(data): end = len(data)\n",
    "        grams = find_ngrams(data[start: end], n) # generates the n-grams\n",
    "        print(\"grams = \",grams)\n",
    "        splits = list(zip(*grams)) # split it\n",
    "        print(\"splits = \", splits)\n",
    "        X += list(zip(*splits[:len(splits)-1])) # from the inputs\n",
    "        print(\"X = \",X)\n",
    "        X = [list(x) for x in X] \n",
    "        print(\"X = \",X)\n",
    "        Y += splits[-1] # form the targets\n",
    "        print(\"Y = \",Y)\n",
    "        if start + buff_size > len(data):\n",
    "            print(\"start + buff_size\")\n",
    "            start = 0\n",
    "            epoch += 1\n",
    "            end = start + buff_size\n",
    "        else:\n",
    "            start = start + buff_size\n",
    "            end = end + buff_size\n",
    "    return dataset_X, dataset_Y \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters# Hyperp \n",
    "LR = 0.0001\n",
    "HIDDEN_DIMS = 256\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS=10\n",
    "N_GRAMS = 5\n",
    "N_VALIDATE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 1, 6, 7, 1, 8, 9, 10, 11, 12, 13]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_id_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "epoch 0\n",
      "len(X) =  0\n",
      "n =  3\n",
      "start,end =  0 8\n",
      "n =  3\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x0000000029C22FC8>\n",
      "splits =  [(2, 3, 4, 5, 1, 6), (3, 4, 5, 1, 6, 7), (4, 5, 1, 6, 7, 1)]\n",
      "X =  [(2, 3), (3, 4), (4, 5), (5, 1), (1, 6), (6, 7)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7]]\n",
      "Y =  [4, 5, 1, 6, 7, 1]\n",
      "epoch 0\n",
      "len(X) =  6\n",
      "n =  3\n",
      "start,end =  8 16\n",
      "n =  3\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x0000000029C4C708>\n",
      "splits =  [(8, 9, 10, 11), (9, 10, 11, 12), (10, 11, 12, 13)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], (8, 9), (9, 10), (10, 11), (11, 12)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 1\n",
      "len(X) =  10\n",
      "n =  4\n",
      "start,end =  0 8\n",
      "n =  4\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x0000000029C2D448>\n",
      "splits =  [(2, 3, 4, 5, 1), (3, 4, 5, 1, 6), (4, 5, 1, 6, 7), (5, 1, 6, 7, 1)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], (2, 3, 4), (3, 4, 5), (4, 5, 1), (5, 1, 6), (1, 6, 7)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 10, 11, 12, 13, 5, 1, 6, 7, 1]\n",
      "epoch 1\n",
      "len(X) =  15\n",
      "n =  2\n",
      "start,end =  8 16\n",
      "n =  2\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x0000000029C3E4C8>\n",
      "splits =  [(8, 9, 10, 11, 12), (9, 10, 11, 12, 13)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], (8,), (9,), (10,), (11,), (12,)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 10, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 2\n",
      "len(X) =  20\n",
      "n =  3\n",
      "start,end =  0 8\n",
      "n =  3\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x0000000029C4C408>\n",
      "splits =  [(2, 3, 4, 5, 1, 6), (3, 4, 5, 1, 6, 7), (4, 5, 1, 6, 7, 1)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], (2, 3), (3, 4), (4, 5), (5, 1), (1, 6), (6, 7)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 10, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 4, 5, 1, 6, 7, 1]\n",
      "epoch 2\n",
      "len(X) =  26\n",
      "n =  4\n",
      "start,end =  8 16\n",
      "n =  4\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x0000000029C57748>\n",
      "splits =  [(8, 9, 10), (9, 10, 11), (10, 11, 12), (11, 12, 13)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], (8, 9, 10), (9, 10, 11), (10, 11, 12)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 10, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 4, 5, 1, 6, 7, 1, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 3\n",
      "len(X) =  29\n",
      "n =  2\n",
      "start,end =  0 8\n",
      "n =  2\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x0000000029C2F4C8>\n",
      "splits =  [(2, 3, 4, 5, 1, 6, 7), (3, 4, 5, 1, 6, 7, 1)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], (2,), (3,), (4,), (5,), (1,), (6,), (7,)]\n",
      "X =  [[2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8], [9], [10], [11], [12], [2, 3], [3, 4], [4, 5], [5, 1], [1, 6], [6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7]]\n",
      "Y =  [4, 5, 1, 6, 7, 1, 10, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1]\n",
      "epoch 3\n",
      "len(X) =  36\n",
      "X_batch =  [[ 0  0  0  2  3]\n",
      " [ 0  0  0  3  4]\n",
      " [ 0  0  0  4  5]\n",
      " [ 0  0  0  5  1]\n",
      " [ 0  0  0  1  6]\n",
      " [ 0  0  0  6  7]\n",
      " [ 0  0  0  8  9]\n",
      " [ 0  0  0  9 10]\n",
      " [ 0  0  0 10 11]\n",
      " [ 0  0  0 11 12]\n",
      " [ 0  0  2  3  4]\n",
      " [ 0  0  3  4  5]\n",
      " [ 0  0  4  5  1]\n",
      " [ 0  0  5  1  6]\n",
      " [ 0  0  1  6  7]\n",
      " [ 0  0  0  0  8]\n",
      " [ 0  0  0  0  9]\n",
      " [ 0  0  0  0 10]\n",
      " [ 0  0  0  0 11]\n",
      " [ 0  0  0  0 12]\n",
      " [ 0  0  0  2  3]\n",
      " [ 0  0  0  3  4]\n",
      " [ 0  0  0  4  5]\n",
      " [ 0  0  0  5  1]\n",
      " [ 0  0  0  1  6]\n",
      " [ 0  0  0  6  7]\n",
      " [ 0  0  8  9 10]\n",
      " [ 0  0  9 10 11]\n",
      " [ 0  0 10 11 12]\n",
      " [ 0  0  0  0  2]\n",
      " [ 0  0  0  0  3]\n",
      " [ 0  0  0  0  4]]\n",
      "Y_batch =  [4, 5, 1, 6, 7, 1, 10, 11, 12, 13, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 4, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5]\n",
      "epoch 3\n",
      "len(X) =  4\n",
      "n =  4\n",
      "start,end =  8 16\n",
      "n =  4\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x0000000029C32588>\n",
      "splits =  [(8, 9, 10), (9, 10, 11), (10, 11, 12), (11, 12, 13)]\n",
      "X =  [[5], [1], [6], [7], (8, 9, 10), (9, 10, 11), (10, 11, 12)]\n",
      "X =  [[5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12]]\n",
      "Y =  [1, 6, 7, 1, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 4\n",
      "len(X) =  7\n",
      "n =  4\n",
      "start,end =  0 8\n",
      "n =  4\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x0000000029C3A0C8>\n",
      "splits =  [(2, 3, 4, 5, 1), (3, 4, 5, 1, 6), (4, 5, 1, 6, 7), (5, 1, 6, 7, 1)]\n",
      "X =  [[5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], (2, 3, 4), (3, 4, 5), (4, 5, 1), (5, 1, 6), (1, 6, 7)]\n",
      "X =  [[5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7]]\n",
      "Y =  [1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1]\n",
      "epoch 4\n",
      "len(X) =  12\n",
      "n =  4\n",
      "start,end =  8 16\n",
      "n =  4\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x0000000029C4AF08>\n",
      "splits =  [(8, 9, 10), (9, 10, 11), (10, 11, 12), (11, 12, 13)]\n",
      "X =  [[5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], (8, 9, 10), (9, 10, 11), (10, 11, 12)]\n",
      "X =  [[5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12]]\n",
      "Y =  [1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 5\n",
      "len(X) =  15\n",
      "n =  2\n",
      "start,end =  0 8\n",
      "n =  2\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x0000000029C3E688>\n",
      "splits =  [(2, 3, 4, 5, 1, 6, 7), (3, 4, 5, 1, 6, 7, 1)]\n",
      "X =  [[5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], (2,), (3,), (4,), (5,), (1,), (6,), (7,)]\n",
      "X =  [[5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7]]\n",
      "Y =  [1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1]\n",
      "epoch 5\n",
      "len(X) =  22\n",
      "n =  3\n",
      "start,end =  8 16\n",
      "n =  3\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x0000000029C3A308>\n",
      "splits =  [(8, 9, 10, 11), (9, 10, 11, 12), (10, 11, 12, 13)]\n",
      "X =  [[5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], (8, 9), (9, 10), (10, 11), (11, 12)]\n",
      "X =  [[5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8, 9], [9, 10], [10, 11], [11, 12]]\n",
      "Y =  [1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 6\n",
      "len(X) =  26\n",
      "n =  2\n",
      "start,end =  0 8\n",
      "n =  2\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grams =  <zip object at 0x0000000029C4AC08>\n",
      "splits =  [(2, 3, 4, 5, 1, 6, 7), (3, 4, 5, 1, 6, 7, 1)]\n",
      "X =  [[5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8, 9], [9, 10], [10, 11], [11, 12], (2,), (3,), (4,), (5,), (1,), (6,), (7,)]\n",
      "X =  [[5], [1], [6], [7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8, 9], [9, 10], [10, 11], [11, 12], [2], [3], [4], [5], [1], [6], [7]]\n",
      "Y =  [1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 10, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1]\n",
      "epoch 6\n",
      "len(X) =  33\n",
      "X_batch =  [[ 0  0  0  0  5]\n",
      " [ 0  0  0  0  1]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  0  7]\n",
      " [ 0  0  8  9 10]\n",
      " [ 0  0  9 10 11]\n",
      " [ 0  0 10 11 12]\n",
      " [ 0  0  2  3  4]\n",
      " [ 0  0  3  4  5]\n",
      " [ 0  0  4  5  1]\n",
      " [ 0  0  5  1  6]\n",
      " [ 0  0  1  6  7]\n",
      " [ 0  0  8  9 10]\n",
      " [ 0  0  9 10 11]\n",
      " [ 0  0 10 11 12]\n",
      " [ 0  0  0  0  2]\n",
      " [ 0  0  0  0  3]\n",
      " [ 0  0  0  0  4]\n",
      " [ 0  0  0  0  5]\n",
      " [ 0  0  0  0  1]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  0  7]\n",
      " [ 0  0  0  8  9]\n",
      " [ 0  0  0  9 10]\n",
      " [ 0  0  0 10 11]\n",
      " [ 0  0  0 11 12]\n",
      " [ 0  0  0  0  2]\n",
      " [ 0  0  0  0  3]\n",
      " [ 0  0  0  0  4]\n",
      " [ 0  0  0  0  5]\n",
      " [ 0  0  0  0  1]\n",
      " [ 0  0  0  0  6]]\n",
      "Y_batch =  [1, 6, 7, 1, 11, 12, 13, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 10, 11, 12, 13, 3, 4, 5, 1, 6, 7]\n",
      "epoch 6\n",
      "len(X) =  1\n",
      "n =  3\n",
      "start,end =  8 16\n",
      "n =  3\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x0000000029C41688>\n",
      "splits =  [(8, 9, 10, 11), (9, 10, 11, 12), (10, 11, 12, 13)]\n",
      "X =  [[7], (8, 9), (9, 10), (10, 11), (11, 12)]\n",
      "X =  [[7], [8, 9], [9, 10], [10, 11], [11, 12]]\n",
      "Y =  [1, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 7\n",
      "len(X) =  5\n",
      "n =  4\n",
      "start,end =  0 8\n",
      "n =  4\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x0000000029C4CF48>\n",
      "splits =  [(2, 3, 4, 5, 1), (3, 4, 5, 1, 6), (4, 5, 1, 6, 7), (5, 1, 6, 7, 1)]\n",
      "X =  [[7], [8, 9], [9, 10], [10, 11], [11, 12], (2, 3, 4), (3, 4, 5), (4, 5, 1), (5, 1, 6), (1, 6, 7)]\n",
      "X =  [[7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7]]\n",
      "Y =  [1, 10, 11, 12, 13, 5, 1, 6, 7, 1]\n",
      "epoch 7\n",
      "len(X) =  10\n",
      "n =  4\n",
      "start,end =  8 16\n",
      "n =  4\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x0000000029C48608>\n",
      "splits =  [(8, 9, 10), (9, 10, 11), (10, 11, 12), (11, 12, 13)]\n",
      "X =  [[7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], (8, 9, 10), (9, 10, 11), (10, 11, 12)]\n",
      "X =  [[7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12]]\n",
      "Y =  [1, 10, 11, 12, 13, 5, 1, 6, 7, 1, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 8\n",
      "len(X) =  13\n",
      "n =  2\n",
      "start,end =  0 8\n",
      "n =  2\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x0000000029C2F8C8>\n",
      "splits =  [(2, 3, 4, 5, 1, 6, 7), (3, 4, 5, 1, 6, 7, 1)]\n",
      "X =  [[7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], (2,), (3,), (4,), (5,), (1,), (6,), (7,)]\n",
      "X =  [[7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7]]\n",
      "Y =  [1, 10, 11, 12, 13, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1]\n",
      "epoch 8\n",
      "len(X) =  20\n",
      "n =  2\n",
      "start,end =  8 16\n",
      "n =  2\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x0000000029C2FC88>\n",
      "splits =  [(8, 9, 10, 11, 12), (9, 10, 11, 12, 13)]\n",
      "X =  [[7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], (8,), (9,), (10,), (11,), (12,)]\n",
      "X =  [[7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12]]\n",
      "Y =  [1, 10, 11, 12, 13, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 9\n",
      "len(X) =  25\n",
      "n =  2\n",
      "start,end =  0 8\n",
      "n =  2\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x0000000029C3EC88>\n",
      "splits =  [(2, 3, 4, 5, 1, 6, 7), (3, 4, 5, 1, 6, 7, 1)]\n",
      "X =  [[7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12], (2,), (3,), (4,), (5,), (1,), (6,), (7,)]\n",
      "X =  [[7], [8, 9], [9, 10], [10, 11], [11, 12], [2, 3, 4], [3, 4, 5], [4, 5, 1], [5, 1, 6], [1, 6, 7], [8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8], [9], [10], [11], [12], [2], [3], [4], [5], [1], [6], [7]]\n",
      "Y =  [1, 10, 11, 12, 13, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1]\n",
      "epoch 9\n",
      "len(X) =  32\n",
      "X_batch =  [[ 0  0  0  0  7]\n",
      " [ 0  0  0  8  9]\n",
      " [ 0  0  0  9 10]\n",
      " [ 0  0  0 10 11]\n",
      " [ 0  0  0 11 12]\n",
      " [ 0  0  2  3  4]\n",
      " [ 0  0  3  4  5]\n",
      " [ 0  0  4  5  1]\n",
      " [ 0  0  5  1  6]\n",
      " [ 0  0  1  6  7]\n",
      " [ 0  0  8  9 10]\n",
      " [ 0  0  9 10 11]\n",
      " [ 0  0 10 11 12]\n",
      " [ 0  0  0  0  2]\n",
      " [ 0  0  0  0  3]\n",
      " [ 0  0  0  0  4]\n",
      " [ 0  0  0  0  5]\n",
      " [ 0  0  0  0  1]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  0  7]\n",
      " [ 0  0  0  0  8]\n",
      " [ 0  0  0  0  9]\n",
      " [ 0  0  0  0 10]\n",
      " [ 0  0  0  0 11]\n",
      " [ 0  0  0  0 12]\n",
      " [ 0  0  0  0  2]\n",
      " [ 0  0  0  0  3]\n",
      " [ 0  0  0  0  4]\n",
      " [ 0  0  0  0  5]\n",
      " [ 0  0  0  0  1]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  0  7]]\n",
      "Y_batch =  [1, 10, 11, 12, 13, 5, 1, 6, 7, 1, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 9, 10, 11, 12, 13, 3, 4, 5, 1, 6, 7, 1]\n",
      "epoch 9\n",
      "len(X) =  0\n",
      "n =  4\n",
      "start,end =  8 16\n",
      "n =  4\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x0000000029C488C8>\n",
      "splits =  [(8, 9, 10), (9, 10, 11), (10, 11, 12), (11, 12, 13)]\n",
      "X =  [(8, 9, 10), (9, 10, 11), (10, 11, 12)]\n",
      "X =  [[8, 9, 10], [9, 10, 11], [10, 11, 12]]\n",
      "Y =  [11, 12, 13]\n",
      "start + buff_size\n",
      "epoch 10\n",
      "len(X) =  3\n",
      "n =  2\n",
      "start,end =  0 8\n",
      "n =  2\n",
      "input_list =  [2, 3, 4, 5, 1, 6, 7, 1]\n",
      "grams =  <zip object at 0x0000000029C32348>\n",
      "splits =  [(2, 3, 4, 5, 1, 6, 7), (3, 4, 5, 1, 6, 7, 1)]\n",
      "X =  [[8, 9, 10], [9, 10, 11], [10, 11, 12], (2,), (3,), (4,), (5,), (1,), (6,), (7,)]\n",
      "X =  [[8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7]]\n",
      "Y =  [11, 12, 13, 3, 4, 5, 1, 6, 7, 1]\n",
      "epoch 10\n",
      "len(X) =  10\n",
      "n =  3\n",
      "start,end =  8 16\n",
      "n =  3\n",
      "input_list =  [8, 9, 10, 11, 12, 13]\n",
      "grams =  <zip object at 0x0000000029C3E688>\n",
      "splits =  [(8, 9, 10, 11), (9, 10, 11, 12), (10, 11, 12, 13)]\n",
      "X =  [[8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], (8, 9), (9, 10), (10, 11), (11, 12)]\n",
      "X =  [[8, 9, 10], [9, 10, 11], [10, 11, 12], [2], [3], [4], [5], [1], [6], [7], [8, 9], [9, 10], [10, 11], [11, 12]]\n",
      "Y =  [11, 12, 13, 3, 4, 5, 1, 6, 7, 1, 10, 11, 12, 13]\n",
      "start + buff_size\n"
     ]
    }
   ],
   "source": [
    "dataset_X, dataset_Y  = prepare_data(train_id_seqs, N_GRAMS, BATCH_SIZE, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from numpy.random import seed\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from IPython import display\n",
    "import time\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle , gc\n",
    "from fractions import Fraction\n",
    "from collections import Counter, OrderedDict\n",
    "# from Activation import relu\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate,Reshape\n",
    "from keras.optimizers import Adagrad, Adam, RMSprop\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D,Conv1D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Bidirectional\n",
    "from keras import activations\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D,MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Input\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 100)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_layer = Embedding(len(Word2Idx),\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=N_GRAMS,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5)\n",
      "(?, 5, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 5, 100)            1900      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 19)                969       \n",
      "=================================================================\n",
      "Total params: 33,069\n",
      "Trainable params: 31,169\n",
      "Non-trainable params: 1,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(N_GRAMS,), dtype='int32')\n",
    "print(sequence_input.shape)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "print(embedded_sequences.shape)\n",
    "model = LSTM(50)(embedded_sequences)\n",
    "preds = Dense(len(Word2Idx), activation='softmax')(model)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 96 arrays: [array([0, 0, 0, 2, 3]), array([0, 0, 0, 3, 4]), array([0, 0, 0, 4, 5]), array([0, 0, 0, 5, 1]), array([0, 0, 0, 1, 6]), array([0, 0, 0, 6, 7]), array([0, 0, 0, 8, 9]), array([ 0,  0,  0,  9, 10]), ar...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-f2174d858ff0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# happy learning!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m model.fit(dataset_X, dataset_Y, validation_data=(dataset_X, dataset_Y),\n\u001b[1;32m----> 3\u001b[1;33m           epochs=2, batch_size=128)\n\u001b[0m",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1555\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1556\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1407\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1410\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1411\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development_Avecto\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     87\u001b[0m                                  \u001b[1;34m'the following list of '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                                  \u001b[1;34m' arrays: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                                  '...')\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 96 arrays: [array([0, 0, 0, 2, 3]), array([0, 0, 0, 3, 4]), array([0, 0, 0, 4, 5]), array([0, 0, 0, 5, 1]), array([0, 0, 0, 1, 6]), array([0, 0, 0, 6, 7]), array([0, 0, 0, 8, 9]), array([ 0,  0,  0,  9, 10]), ar..."
     ]
    }
   ],
   "source": [
    "# happy learning!\n",
    "model.fit(dataset_X, dataset_Y, validation_data=(dataset_X, dataset_Y),\n",
    "          epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
